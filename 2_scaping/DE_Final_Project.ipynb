{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = \"https://cuir.car.chula.ac.th\"\n",
    "\n",
    "# Set headers to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_home_page( collection_link , last_offset ) :\n",
    "    # Initialize the starting value\n",
    "    offset = 0\n",
    "    Thesis_2013 = 0\n",
    "    Thesis_2014 = 0\n",
    "    Thesis_2015 = 0\n",
    "    Thesis_2016 = 0\n",
    "    Thesis_2017 = 0\n",
    "    Thesis_2018 = 0\n",
    "    Thesis_2019 = 0\n",
    "    Thesis_2020 = 0\n",
    "    Thesis_2021 = 0\n",
    "    Thesis_2022 = 0\n",
    "    Thesis_2023 = 0\n",
    "    \n",
    "    lock = threading.Lock()\n",
    "\n",
    "\n",
    "    # Base URL template\n",
    "    url_template = '{collection_link}?offset={offset}'\n",
    "\n",
    "    # Function to process each URL\n",
    "    def process_page(offset):\n",
    "        nonlocal Thesis_2013, Thesis_2014, Thesis_2015, Thesis_2016, Thesis_2017, Thesis_2018, Thesis_2019, Thesis_2020, Thesis_2021, Thesis_2022, Thesis_2023  # Ensure we modify the outer variables\n",
    "        url = f'{collection_link}?offset={offset}'\n",
    "\n",
    "        # try:\n",
    "            # Make the HTTP GET request\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()  # Raise an error for HTTP codes 4xx/5xx\n",
    "\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "        # Extract year data and links using the appropriate selector\n",
    "        years = soup.find_all('td', headers='t1')  \n",
    "\n",
    "        if years:\n",
    "            # Iterate over the years\n",
    "            for year in years:\n",
    "                year_text = year.text.strip()\n",
    "                # Collect counts for the relevant years\n",
    "                with lock:  # Acquire the lock to safely modify shared variables\n",
    "                    if year_text == '2013' or year_text == '2556':\n",
    "                        Thesis_2013 += 1\n",
    "                    elif year_text == '2014' or year_text == '2557':\n",
    "                        Thesis_2014 += 1\n",
    "                    elif year_text == '2015' or year_text == '2558':\n",
    "                        Thesis_2015 += 1\n",
    "                    elif year_text == '2016' or year_text == '2559':\n",
    "                        Thesis_2016 += 1\n",
    "                    elif year_text == '2017' or year_text == '2560':\n",
    "                        Thesis_2017 += 1\n",
    "                    elif year_text == '2018' or year_text == '2561':\n",
    "                        Thesis_2018 += 1\n",
    "                    elif year_text == '2019' or year_text == '2562':\n",
    "                        Thesis_2019 += 1\n",
    "                    elif year_text == '2020' or year_text == '2563':\n",
    "                        Thesis_2020 += 1\n",
    "                    elif year_text == '2021' or year_text == '2564':\n",
    "                        Thesis_2021 += 1\n",
    "                    elif year_text == '2022' or year_text == '2565':\n",
    "                        Thesis_2022 += 1\n",
    "                    elif year_text == '2023' or year_text == '2566':\n",
    "                        Thesis_2023 += 1\n",
    "                # print(f\"Finished offset: {offset}\")\n",
    "\n",
    "            # else:\n",
    "            #     print(\"No year or link data found on the current page.\")\n",
    "\n",
    "        # except requests.exceptions.RequestException as e:\n",
    "            # print(f\"Error fetching URL at offset {offset}: {e}\")\n",
    "\n",
    "    # Number of worker threads to use\n",
    "    max_workers = 10\n",
    "\n",
    "    # Use ThreadPoolExecutor to run the process_page function concurrently\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Main batch processing\n",
    "        while offset + 20 * max_workers <= last_offset:\n",
    "            # Submit tasks to the executor for the current batch of offsets\n",
    "            futures = [executor.submit(process_page, offset + i * 20) for i in range(max_workers)]\n",
    "            offset += 20 * max_workers  # Increment offset for the next batch of requests\n",
    "\n",
    "            # Wait for all futures in the current batch to finish\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "        # print(\" ------------------------- LEFT -------------------------\")\n",
    "\n",
    "        # Handle the remaining offsets (less than one batch)\n",
    "        while offset < last_offset:\n",
    "            futures = [executor.submit(process_page, offset)]\n",
    "            offset += 20  # Increment offset for each remaining page\n",
    "\n",
    "            # Wait for the single future to finish\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "    # After the loop, print the count of thesis by year\n",
    "    # print(falculty_name)\n",
    "    # # print(offset)\n",
    "    # print(f\"Thesis in 2015: {Thesis_2015}\")\n",
    "    # print(f\"Thesis in 2016: {Thesis_2016}\")\n",
    "    # print(f\"Thesis in 2017: {Thesis_2017}\")\n",
    "    \n",
    "    return( Thesis_2013 , Thesis_2014 , Thesis_2015 , Thesis_2016 , Thesis_2017 , Thesis_2018 , Thesis_2019 , Thesis_2020 , Thesis_2021 , Thesis_2022 , Thesis_2023 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def falculty_page( falculty_link ) :\n",
    "    \n",
    "    # Make the request to fetch the page\n",
    "    response = requests.get(falculty_link, headers=headers)\n",
    "    response.raise_for_status()  # Raise an error for HTTP codes 4xx/5xx\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    # Find the <h4> tag with class \"list-group-item-heading\"\n",
    "    collections = soup.find_all('h4', class_='list-group-item-heading')\n",
    "    links = soup.find_all('h4', class_='list-group-item-heading')\n",
    "    \n",
    "    all_thesis_2013 = 0\n",
    "    all_thesis_2014 = 0\n",
    "    all_thesis_2015 = 0\n",
    "    all_thesis_2016 = 0\n",
    "    all_thesis_2017 = 0\n",
    "    all_thesis_2018 = 0\n",
    "    all_thesis_2019 = 0\n",
    "    all_thesis_2020 = 0\n",
    "    all_thesis_2021 = 0\n",
    "    all_thesis_2022 = 0\n",
    "    all_thesis_2023 = 0\n",
    "\n",
    "    # Use regular expression to find the number in square brackets\n",
    "    for collection , link in zip(collections , links) :\n",
    "        last_offset_uncleaned = re.search(r'\\[(\\d+)\\]', collection.text.strip())\n",
    "\n",
    "\n",
    "        if last_offset_uncleaned:\n",
    "            # If a match is found, print the number inside the brackets\n",
    "            last_offset = int(last_offset_uncleaned.group(1))\n",
    "            # print(last_offset)\n",
    "        \n",
    "            \n",
    "        if last_offset > 0: \n",
    "            anchor = link.find('a', href=True)\n",
    "            collection_link = urljoin(base_url, anchor['href'])\n",
    "            t13 , t14 , t15 , t16 , t17 , t18 , t19 , t20, t21 , t22 , t23= collection_home_page( collection_link , last_offset)\n",
    "            all_thesis_2013 += t13\n",
    "            all_thesis_2014 += t14\n",
    "            all_thesis_2015 += t15\n",
    "            all_thesis_2016 += t16\n",
    "            all_thesis_2017 += t17\n",
    "            all_thesis_2018 += t18\n",
    "            all_thesis_2019 += t19\n",
    "            all_thesis_2020 += t20\n",
    "            all_thesis_2021 += t21\n",
    "            all_thesis_2022 += t22\n",
    "            all_thesis_2023 += t23\n",
    "\n",
    "    return  all_thesis_2013 , all_thesis_2014 , all_thesis_2015 , all_thesis_2016 , all_thesis_2017 , all_thesis_2018 , all_thesis_2019 , all_thesis_2020 , all_thesis_2021 , all_thesis_2022 , all_thesis_2023\n",
    "            # print(last_offset)\n",
    "        # else:\n",
    "        #     # print(\" No Data \" )\n",
    "    \n",
    "\n",
    "    \n",
    "    # return falculty_name , falculty_link\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faculty: วิทยาลัยประชากรศาสตร์, Result: (10, 9, 14, 6, 6, 10, 5, 2, 8, 8, 1)\n",
      "Faculty: วิทยาลัยวิทยาศาสตร์สาธารณสุข, Result: (50, 40, 41, 47, 55, 35, 35, 29, 22, 11, 4)\n",
      "Faculty: คณะสหเวชศาสตร์, Result: (11, 13, 29, 20, 19, 8, 23, 20, 7, 10, 4)\n",
      "Faculty: คณะสถาปัตยกรรมศาสตร์, Result: (87, 87, 83, 114, 81, 76, 73, 77, 59, 60, 13)\n",
      "Faculty: คณะอักษรศาสตร์, Result: (96, 95, 97, 89, 74, 48, 36, 60, 51, 51, 28)\n",
      "Faculty: คณะพาณิชยศาสตร์และการบัญชี, Result: (40, 35, 47, 29, 28, 21, 60, 51, 78, 106, 50)\n",
      "Faculty: คณะนิเทศศาสตร์, Result: (71, 57, 61, 33, 43, 28, 72, 73, 72, 70, 14)\n",
      "Faculty: คณะทันตแพทยศาสตร์, Result: (26, 27, 29, 40, 38, 39, 42, 49, 31, 27, 4)\n",
      "Faculty: คณะเศรษฐศาสตร์, Result: (39, 48, 33, 31, 32, 32, 63, 84, 56, 37, 7)\n",
      "Faculty: คณะครุศาสตร์, Result: (216, 227, 267, 241, 198, 164, 158, 200, 191, 120, 40)\n",
      "Faculty: คณะวิศวกรรมศาสตร์, Result: (452, 404, 409, 371, 297, 301, 316, 254, 238, 218, 72)\n",
      "Faculty: คณะศิลปกรรมศาสตร์, Result: (42, 46, 47, 42, 57, 70, 61, 65, 57, 63, 12)\n",
      "Faculty: คณะนิติศาสตร์, Result: (93, 53, 85, 76, 101, 89, 116, 134, 103, 94, 9)\n",
      "Faculty: คณะแพทยศาสตร์, Result: (116, 140, 118, 133, 134, 139, 115, 103, 108, 89, 19)\n",
      "Faculty: คณะพยาบาลศาสตร์, Result: (112, 96, 105, 106, 89, 64, 37, 33, 27, 37, 20)\n",
      "Faculty: คณะเภสัชศาสตร์, Result: (62, 60, 60, 55, 44, 33, 49, 37, 45, 26, 7)\n",
      "Faculty: คณะรัฐศาสตร์, Result: (41, 71, 37, 29, 37, 64, 138, 188, 153, 156, 68)\n",
      "Faculty: คณะจิตวิทยา, Result: (66, 55, 51, 69, 25, 23, 23, 30, 31, 29, 9)\n",
      "Faculty: คณะวิทยาศาสตร์, Result: (312, 411, 449, 418, 374, 527, 547, 497, 171, 121, 29)\n",
      "Faculty: คณะวิทยาศาสตร์การกีฬา, Result: (64, 48, 44, 38, 42, 54, 40, 31, 22, 20, 8)\n",
      "Faculty: คณะสัตวแพทยศาสตร์, Result: (40, 48, 45, 35, 37, 30, 35, 33, 27, 21, 6)\n",
      "Faculty: บัณฑิตวิทยาลัย, Result: (198, 157, 140, 134, 92, 110, 150, 201, 191, 177, 38)\n",
      "Faculty: วิทยานิพนธ์และสารนิพนธ์ใหม่ จากระบบบริหารจัดการวิทยานิพนธ์ (CU iThesis), Result: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2)\n",
      "Faculty: สถาบันเอเชียศึกษา, Result: (2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0)\n",
      "Faculty: สถาบันเทคโนโลยีชีวภาพและวิศวกรรมพันธุศาสตร์, Result: (3, 10, 11, 4, 0, 6, 0, 0, 0, 0, 0)\n",
      "Faculty: สถาบันไทยศึกษา, Result: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Faculty: สถาบันภาษา, Result: (0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0)\n",
      "Faculty: สำนักงานวิทยทรัพยากร (หอสมุดกลาง), Result: (9, 5, 5, 1, 1, 1, 2, 1, 1, 0, 0)\n",
      "Faculty: วิทยาลัยปิโตรเลียมและปิโตรเคมี, Result: (96, 112, 85, 5, 24, 33, 9, 9, 2, 1, 0)\n",
      "Faculty: สถาบันบัณฑิตบริหารธุรกิจศศินทร์, Result: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Transposed CSV file 'transposed_thesis_summary.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the main page\n",
    "url = 'https://cuir.car.chula.ac.th/handle/123456789/4587'  # Replace with the actual URL\n",
    "\n",
    "# Make the request to fetch the page\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Raise an error for HTTP codes 4xx/5xx\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# Find all the faculty descriptions and links\n",
    "facs = soup.find_all('p', class_='collectionDescription')\n",
    "links = soup.find_all('h4', class_='list-group-item-heading')\n",
    "\n",
    "# Prepare CSV structure\n",
    "output_file = \"thesis_summary.csv\"\n",
    "# columns = [\"Year\"]\n",
    "results_dict = {\"2013\": [], \"2014\": [], \"2015\": [], \"2016\": [], \"2017\": [], \"2018\": [], \"2019\": [], \"2020\": [], \"2021\": [], \"2022\": [], \"2023\": []}\n",
    "\n",
    "# Transposed output file\n",
    "transposed_file = \"transposed_thesis_summary.csv\"\n",
    "transposed_rows = []  # To store the transposed rows\n",
    "\n",
    "faculty = ['College of Population Studies',\n",
    "            'College of Public Health Sciences',\n",
    "            'Faculty of Allied Health Sciences',\n",
    "            'Faculty of Architecture',\n",
    "            'Faculty of Arts',\n",
    "            'Faculty of Commerce and Accountancy',\n",
    "            'Faculty of Communication Arts',\n",
    "            'Faculty of Dentistry',\n",
    "            'Faculty of Economics',\n",
    "            'Faculty of Education',\n",
    "            'Faculty of Engineering',\n",
    "            'Faculty of Fine and Applied Arts',\n",
    "            'Faculty of Law',\n",
    "            'Faculty of Medicine',\n",
    "            'Faculty of Nursing',\n",
    "            'Faculty of Pharmaceutical Science',\n",
    "            'Faculty of Political Science',\n",
    "            'Faculty of Psychology',\n",
    "            'Faculty of Science',\n",
    "            'Faculty of Sports Science',\n",
    "            'Faculty of Veterinary Science',\n",
    "            'Graduate School',\n",
    "            'Office of Research Affairs',\n",
    "            'Institute of Asian Studies',\n",
    "            'The Institute of Biotechnology and Genetic Engineering',\n",
    "            'Institute of Thai Studies',\n",
    "            'Language Institute',\n",
    "            'Office of Academic Resources',\n",
    "            'The Petroleum and Petrochemical College',\n",
    "            'Sasin Graduate Institute of Business Administration']\n",
    "faculty_idx = 0\n",
    "# Iterate over faculties and fetch results\n",
    "for fac, link in zip(facs, links):\n",
    "    anchor = link.find('a', href=True)\n",
    "    full_link = urljoin(base_url, anchor['href'])\n",
    "    \n",
    "    # Call the faculty_page function\n",
    "    # faculty_name = anchor.text.split(\" - \")[0].strip()\n",
    "    result = falculty_page(full_link)\n",
    "    print(f\"Faculty: {fac.text.strip()}, Result: {result}\")\n",
    "    \n",
    "    # Prepare a row for transposed CSV (faculty name + thesis data)\n",
    "    faculty_row = []\n",
    "    faculty_row.append(faculty[faculty_idx])\n",
    "    # Add faculty name to the front of the row\n",
    "    faculty_row.extend(result[:12])  # Appends result[0] to result[11] (first 12 elements)\n",
    "\n",
    "    \n",
    "    faculty_idx += 1\n",
    "    # Append the faculty row to the transposed rows\n",
    "    transposed_rows.append(faculty_row)\n",
    "\n",
    "# Write transposed data to CSV\n",
    "with open(transposed_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # Write the header (first column is faculty names and subsequent columns are years)\n",
    "    writer.writerow([\"Faculty Names\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"])\n",
    "\n",
    "    # Write each transposed row (faculty + thesis data)\n",
    "    writer.writerows(transposed_rows)\n",
    "\n",
    "print(f\"Transposed CSV file '{transposed_file}' created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
